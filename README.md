# PDF Chat App with AWS Bedrock and FAISS Vector Search

This project enables users to upload PDF documents, index their content using embeddings stored in a FAISS vector store, and then interactively chat with the indexed PDFs using an AI language model powered by AWS Bedrock.

---

## Project Structure

- **Admin** (`admin.py`):  
  A Streamlit web app interface to upload PDFs, split and embed their content using AWS Bedrock embeddings, store the FAISS index locally, and upload the index files to an S3 bucket.

- **User** (`app.py`):  
  A Streamlit app where users can input a document ID (generated by the Admin app), load the corresponding vector index from S3, and ask questions about the document. The app retrieves relevant chunks and uses AWS Bedrock Claude v2 chat model to generate concise answers.

---

## Key Technologies Used

- **Streamlit** — Web UI framework for Python, providing interactive user interfaces.
- **LangChain** — Orchestrates the language model, embeddings, vector search, and prompt chains.
- **FAISS** — Fast similarity search engine for efficient retrieval of document chunks.
- **AWS Bedrock** — Provides LLM and embedding models (e.g., Amazon Titan embeddings, Anthropic Claude) via the Bedrock API.
- **Boto3** — AWS SDK for Python to interact with S3 for storing and retrieving indexed data.
- **PyPDFLoader** — For loading and splitting PDFs into manageable text chunks.
- **Docker** — Containerization for consistent deployment of Admin and User apps.

---

## How It Works

1. **Admin Upload and Indexing:**
   - Upload PDF via Admin UI.
   - PDF is split into overlapping text chunks.
   - Each chunk is embedded using AWS Bedrock embeddings.
   - FAISS vector store is built from these embeddings.
   - The FAISS index files are uploaded to an S3 bucket with a generated unique document ID.

2. **User Interaction:**
   - User enters the document ID into the User UI.
   - The app downloads the FAISS index files from S3.
   - User asks questions about the PDF content.
   - Relevant chunks are retrieved from the FAISS index.
   - The Bedrock Claude chat model answers based on retrieved chunks.

---

## Prerequisites

- AWS account with access to Bedrock service and S3.
- Set an S3 bucket for storing FAISS indices.
- Python 3.11 installed (or use Docker).
- Docker (optional, recommended for easier setup).

---

## Tuning the Apps

### Admin App

- Adjust PDF chunk size and overlap in `admin.py` under `RecursiveCharacterTextSplitter` (e.g., `chunk_size=500`, `chunk_overlap=200`) to balance granularity and performance.
- Change the embedding model by updating `BedrockEmbeddings(model_id=...)`.

### User App

- Modify the LLM parameters in `app.py` inside `build_qa_chain_from_vectorstore()` (e.g., temperature, max_tokens) to control answer creativity and length.
- Change number of retrieved chunks (`k`) in `vectorstore.as_retriever(search_kwargs={"k": 5})` to tune retrieval breadth.

---

## Environment Variables

Set the following environment variable for both Admin and User apps before running:

```bash
export BUCKET_NAME="your-s3-bucket-name"

